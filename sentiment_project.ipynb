{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# for building model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Flatten, Dense\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "\n",
    "# for Padding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# for Tokenization \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# NLTK imports\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For regular expressions\n",
    "import re\n",
    "\n",
    "# For data preprocessing\n",
    "from string import punctuation, digits\n",
    "\n",
    "# loading progress\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset shape >> (41157, 6)\n",
      "test dataset shape >> (3798, 6)\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "train_df = pd.read_csv('dataset/Corona_NLP_train.csv', encoding=\"Latin-1\")\n",
    "test_df = pd.read_csv('dataset/Corona_NLP_test.csv', encoding=\"Latin-1\")\n",
    "\n",
    "print(f\"train dataset shape >> {train_df.shape}\")\n",
    "print(f\"test dataset shape >> {test_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18046\n",
      "15398\n",
      "7713\n"
     ]
    }
   ],
   "source": [
    "# Check if dataset will ba balanced after merging Extremely Positive with Positive and so on...\n",
    "print(len(train_df[train_df['Sentiment'] == \"Extremely Positive\"] + train_df[train_df['Sentiment'] == \"Positive\"]))\n",
    "print(len(train_df[train_df['Sentiment'] == \"Extremely Negative\"] + train_df[train_df['Sentiment'] == \"Negative\"]))\n",
    "print(len(train_df[train_df['Sentiment'] == \"Neutral\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat new dataset (which will consist from 2 columns: label, data)\n",
    "\n",
    "def data_label_split(dataset):\n",
    "    data = dataset['OriginalTweet']\n",
    "    label = dataset['Sentiment']\n",
    "    return data,label\n",
    "\n",
    "train_data,train_label = data_label_split(train_df)\n",
    "test_data,test_label = data_label_split(test_df)\n",
    "\n",
    "train = pd.DataFrame({\n",
    "    'label':train_label,\n",
    "    'data':train_data\n",
    "})\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    'label':test_label,\n",
    "    'data':test_data\n",
    "})\n",
    "\n",
    "# Merge some sentiments\n",
    "\n",
    "def reassign_label(x):\n",
    "    if x == \"Extremely Positive\" or x == \"Positive\":\n",
    "        return 1\n",
    "    elif x ==\"Extremely Negative\" or x ==\"Negative\":\n",
    "        return -1\n",
    "    elif x ==\"Neutral\":\n",
    "        return 0\n",
    "\n",
    "train.label = train.label.apply(lambda x:reassign_label(x))\n",
    "test.label = test.label.apply(lambda x:reassign_label(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    list_punctuation = list(punctuation)\n",
    "    for i in list_punctuation:\n",
    "        s = s.replace(i,'')\n",
    "    return s.lower()\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'(\\W)\\1{2,}', r'\\1', sentence) \n",
    "    sentence = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', sentence)\n",
    "    sentence = re.sub(r'(?P<url>https?://[^\\s]+)', '', sentence) # remove URL adresses\n",
    "    sentence = re.sub(r\"\\@(\\w+)\", '', sentence) # remove usernames\n",
    "    sentence = re.sub(r\"\\#(\\w+)\", '', sentence) # remove hashtags\n",
    "    sentence = re.sub(r\"\\$(\\w+)\", '', sentence) # remove cashtags\n",
    "    sentence = sentence.replace(\"-\",' ')\n",
    "    tokens = sentence.split()\n",
    "    tokens = [remove_punctuation(w) for w in tokens] # remove punctuations\n",
    "    stop_words = set(stopwords.words('english')) # remove stopwords\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    tokens = [w.translate(remove_digits) for w in tokens]\n",
    "    tokens = [w.strip() for w in tokens]\n",
    "    tokens = [w for w in tokens if w!=\"\"]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# Another function for post processing that was not used in this research\n",
    "def process_text(text):\n",
    "  text = str(text) #Convert string to str\n",
    "  #Lowers the string\n",
    "  text = text.lower()\n",
    "  #Removes the full url\n",
    "  url_remove = re.compile(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?')\n",
    "  text = re.sub(url_remove,' ',text)\n",
    "  #Removes the punctuation\n",
    "  text = ''.join([string for string in text if string not in punctuation and not string.isdigit()])\n",
    "  #Removes any more special characters\n",
    "  special_character = re.compile(r'[^a-zA-Z]')\n",
    "  text = re.sub(special_character,' ', text)\n",
    "  text = text.strip() #Strip white spaces\n",
    "  text = text.split(' ')\n",
    "  text = ' '.join([string for string in text if string not in stopwords.words('english')])#Removing all stop words\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean sentences in train and test data\n",
    "train.data = train.data.apply(lambda sentence:clean_sentence(sentence))\n",
    "test.data = test.data.apply(lambda sentence:clean_sentence(sentence))\n",
    "\n",
    "#train.data = train.data.apply(process_text)\n",
    "#test.data = test.data.apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data to train and test\n",
    "\n",
    "# Train data\n",
    "train_data = train.data\n",
    "train_label = train.label\n",
    "\n",
    "# Test data\n",
    "test_data = test.data\n",
    "test_label = test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variable (in our case: -1, 0, 1) into dummy/indicator variables. Such as -1 to 1 0 0 \n",
    "train_label = pd.get_dummies(train_label)\n",
    "test_label = pd.get_dummies(test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatize_sentence(de_punct_sent):\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(de_punct_sent))\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lem = [lemmatize_sentence(i) for i in train_data]\n",
    "test_lem = [lemmatize_sentence(i) for i in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32445 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "samples_1 = train_lem\n",
    "samples_2 = test_lem\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(samples_1) # Create an index of all words from training data\n",
    "\n",
    "# Convert train set to sequences\n",
    "train_data = tokenizer.texts_to_sequences(samples_1)\n",
    "# Convert test set to sequences\n",
    "test_data = tokenizer.texts_to_sequences(samples_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.99514055932163"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the good number for length of each sequence\n",
    "\n",
    "lens =  [len(s) for s in train_data]\n",
    "100 - sum(map(lambda x: x > 40, lens))/len(lens)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 40 # maximal length of sentences\n",
    "\n",
    "train_data = pad_sequences(train_data, maxlen=maxlen, padding='post', truncating='post') \n",
    "test_data = pad_sequences(test_data, maxlen=maxlen, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "URLs = [\"http://nlp.stanford.edu/data/glove.42B.300d.zip\", \"https://nlp.stanford.edu/data/glove.6B.zip\"]\n",
    "GLOVES = ['glove.6B.100d.txt', 'glove.42B.300d.txt']\n",
    "\n",
    "def fetch_data(url, target_file=os.path.join('embeddings','glove.zip'), delete_zip=True):\n",
    "    current_directory = os.getcwd()\n",
    "    if not os.path.isdir(os.path.join(current_directory, \"embeddings\")):\n",
    "        os.mkdir(os.path.join(current_directory, \"embeddings\"))\n",
    "    \n",
    "    if os.path.isfile(target_file):\n",
    "        print(\"datasets already downloded :) \")\n",
    "    else:\n",
    "        response = requests.get(url, stream=True)\n",
    "        handle = open(target_file, \"wb\")\n",
    "        for chunk in tqdm.tqdm(response.iter_content(chunk_size=512)):\n",
    "            if chunk:  \n",
    "                handle.write(chunk)\n",
    "        handle.close()  \n",
    "        print(\"  Download completed ;) :\") \n",
    "    \n",
    "    #extract zip_file\n",
    "    with zipfile.ZipFile(target_file, 'r') as zip_ref:\n",
    "        for zipinfo in zip_ref.infolist():\n",
    "            print(\"1. Extracting {} file\".format(target_file))\n",
    "            if (os.path.isfile(os.path.join('embeddings', zipinfo.filename))):\n",
    "                continue\n",
    "            zip_ref.extract(zipinfo, \"embeddings\")\n",
    "    \n",
    "    if delete_zip:\n",
    "        print(\"2. Deleting {} file\".format(target_file))\n",
    "        os.remove(path=target_file)\n",
    "\n",
    "## ------------------------------------------------------------------------------- ##        \n",
    "## if word embeddings need to be downloaded just uncomment all the following lines ##\n",
    "## ------------------------------------------------------------------------------- ##        \n",
    "\n",
    "# fetch_data(\"https://nlp.stanford.edu/data/glove.6B.zip\")\n",
    "# fetch_data(\"http://nlp.stanford.edu/data/glove.42B.300d.zip\")\n",
    "# fetch_data(\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\")\n",
    "# fetch_data(\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip\")\n",
    "# fetch_data(\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip\")\n",
    "# fetch_data(\"https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip\")\n",
    "# fetch_data(\"https://nlp.stanford.edu/data/glove.840B.300d.zip\")\n",
    "# fetch_data(\"https://nlp.stanford.edu/data/glove.twitter.27B.zip\")\n",
    "\n",
    "# link to download gogle word embeddings - https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?resourcekey=0-wjGZdNAUop6WykTtMip30g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for various word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_vectors(fname, skip_first):\n",
    "    embedding_dict = {}\n",
    "    with open(fname, 'r', encoding='utf-8') as f:\n",
    "        if skip_first:\n",
    "            next(f)\n",
    "        for line in f:\n",
    "            values=line.split()\n",
    "            word=values[0]\n",
    "            if word in word_index.keys():\n",
    "                # try, except since some embeddings have white spaces in them for some reason\n",
    "                # we skip those\n",
    "                try:\n",
    "                    vector = np.asarray(values[1:], 'float32')\n",
    "                    embedding_dict[word] = vector\n",
    "                except:\n",
    "                    print(\"An incorrect vector was found and skipped\")\n",
    "    return embedding_dict\n",
    "\n",
    "\n",
    "def construct_zero_embedding_matrix(emb_length, word_index=word_index):\n",
    "    num_words=len(word_index)+1\n",
    "    return np.zeros((num_words, emb_length))\n",
    "\n",
    "def construct_uniform_embeddin_matrix(emb_length, word_index=word_index):\n",
    "    num_words=len(word_index)+1\n",
    "    return  np.random.uniform(low=-1, high=1, size=(num_words, emb_length))\n",
    "\n",
    "def construct_embedding_matrix(fname, emb_length, word_index=word_index, skip_first=False):\n",
    "    embedding_dict = load_pretrained_vectors(fname, skip_first)\n",
    "\n",
    "    num_words=len(word_index)+1\n",
    "    embedding_matrix=np.zeros((num_words, emb_length))\n",
    "\n",
    "    for word,i in tqdm.tqdm(word_index.items()):\n",
    "        if i < num_words:\n",
    "            vect=embedding_dict.get(word, [])\n",
    "            if len(vect)>0:\n",
    "                embedding_matrix[i] = vect[:emb_length]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(embedding_dim, embedding_matrix, vocab_inp_size=len(word_index)+1, hidden_size=256, trainable=True):\n",
    "        model = Sequential() \n",
    "        model.add(Embedding(input_dim = vocab_inp_size, output_dim = embedding_dim, input_length = maxlen, embeddings_initializer=Constant(embedding_matrix), trainable=trainable))\n",
    "        model.add(LSTM(hidden_size, return_sequences=True, dropout=0.4, recurrent_dropout=0.4)) \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        model.summary()\n",
    "        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy']) \n",
    "        return model\n",
    "\n",
    "def train_model(name, embedding_dim, embedding_matrix, vocab_inp_size=len(word_index)+1, hidden_size=256, trainable=True):\n",
    "        model = create_model(embedding_dim, embedding_matrix, vocab_inp_size, hidden_size, trainable=trainable)\n",
    "        early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "        bst_model_path = name + '.h5'\n",
    "        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True)\n",
    "\n",
    "        model.fit(train_data, train_label, \\\n",
    "                epochs=20, batch_size=128, validation_split=0.2, \\\n",
    "                callbacks=[early_stopping, model_checkpoint, CSVLogger(f\"history_{name}.csv\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training - baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with fine-tuning\n",
    "embedding_matrix_baseline_100d = construct_uniform_embeddin_matrix(100)\n",
    "train_model(\"simple_lstm_baseline_100d\", 100, embedding_matrix_baseline_100d)\n",
    "\n",
    "# without fine-tuning\n",
    "embedding_matrix_baseline_100d = construct_uniform_embeddin_matrix(100)\n",
    "train_model(\"simple_lstm_baseline_100d_frozen\", 100, embedding_matrix_baseline_100d, trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training - pre-trained embeddings\n",
    "Caution: Uncomment and run this code only if you want to train all the models. IT CAN TAKE SOME TIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with fine-tuning\n",
    "\n",
    "# embedding_matrix_glove_6b_50d =  construct_embedding_matrix(\"embeddings/glove.6B.50d.txt\", 50)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_50d\", 50, embedding_matrix_glove_6b_50d)\n",
    "\n",
    "# embedding_matrix_glove_6b_100d =  construct_embedding_matrix(\"embeddings/glove.6B.100d.txt\", 100)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_100d\", 100, embedding_matrix_glove_6b_100d)\n",
    "\n",
    "# embedding_matrix_glove_6b_200d =  construct_embedding_matrix(\"embeddings/glove.6B.200d.txt\", 200)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_200d\", 200, embedding_matrix_glove_6b_200d)\n",
    "\n",
    "# embedding_matrix_glove_6b_300d =  construct_embedding_matrix(\"embeddings/glove.6B.300d.txt\", 300)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_300d\", 300, embedding_matrix_glove_6b_300d)\n",
    "\n",
    "\n",
    "# embedding_matrix_glove_42b_300d =  construct_embedding_matrix(\"embeddings/glove.42B.300d.txt\", 300)\n",
    "# train_model(\"simple_lstm_glove_vectors_42B_300d\", 300, embedding_matrix_glove_42b_300d)\n",
    "# embedding_matrix_glove_840b_300d =  construct_embedding_matrix(\"embeddings/glove.840B.300d.txt\", 300)\n",
    "# train_model(\"simple_lstm_glove_vectors_840B_300d\", 300, embedding_matrix_glove_840b_300d)\n",
    "\n",
    "# embedding_matrix_glove_27b_25d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.25d.txt\", 25)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_25d\", 25, embedding_matrix_glove_27b_25d)\n",
    "# embedding_matrix_glove_27b_50d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.50d.txt\", 50)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_50d\", 50, embedding_matrix_glove_27b_50d)\n",
    "# embedding_matrix_glove_27b_100d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.100d.txt\", 100)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_100d\", 100, embedding_matrix_glove_27b_100d)\n",
    "# embedding_matrix_glove_27b_200d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.200d.txt\", 200)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_200d\", 200, embedding_matrix_glove_27b_200d)\n",
    "\n",
    "# embedding_matrix_ff_600B_300d = construct_embedding_matrix_from_ff(\"embeddings/crawl-300d-2M.vec\", 300, True)\n",
    "# train_model(\"simple_lstm_ff_vectors_600B_300d\", 300, embedding_matrix_ff_600B_300d)\n",
    "\n",
    "# embedding_matrix_ff_subwords_600B_300d = construct_embedding_matrix_from_ff(\"embeddings/crawl-300d-2M-subword.vec\", 300, True)\n",
    "# train_model(\"simple_lstm_ff_subwords_vectors_600B_300d\", 300, embedding_matrix_ff_subwords_600B_300d)\n",
    "\n",
    "# embedding_matrix_ff_wiki_16B_300d = construct_embedding_matrix_from_ff(\"embeddings/wiki-news-300d-1M.vec\", 300, True)\n",
    "# train_model(\"simple_lstm_ff_wiki_vectors_16B_300d\", 300, embedding_matrix_ff_wiki_16B_300d)\n",
    "\n",
    "# embedding_matrix_ff_wiki_subwords_16B_300d = construct_embedding_matrix_from_ff(\"embeddings/wiki-news-300d-1M-subword.vec\", 300, True)\n",
    "# train_model(\"simple_lstm_ff_wiki_subwords_vectors_16B_300d\", 300, embedding_matrix_ff_wiki_subwords_16B_300d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without fine-tuning\n",
    "\n",
    "# embedding_matrix_glove_6b_50d =  construct_embedding_matrix(\"embeddings/glove.6B.50d.txt\", 50)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_50d\", 50, embedding_matrix_glove_6b_50d, False)\n",
    "# embedding_matrix_glove_6b_100d =  construct_embedding_matrix(\"embeddings/glove.6B.100d.txt\", 100)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_100d\", 100, embedding_matrix_glove_6b_100d, False)\n",
    "# embedding_matrix_glove_6b_200d =  construct_embedding_matrix(\"embeddings/glove.6B.200d.txt\", 200)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_200d\", 200, embedding_matrix_glove_6b_200d, False)\n",
    "# embedding_matrix_glove_6b_300d =  construct_embedding_matrix(\"embeddings/glove.6B.300d.txt\", 300)\n",
    "# train_model(\"simple_lstm_glove_vectors_6B_300d\", 300, embedding_matrix_glove_6b_300d, False)\n",
    "\n",
    "# embedding_matrix_glove_27b_25d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.25d.txt\", 25)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_25d\", 25, embedding_matrix_glove_27b_25d, False)\n",
    "# embedding_matrix_glove_27b_50d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.50d.txt\", 50)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_50d\", 50, embedding_matrix_glove_27b_50d, False)\n",
    "# embedding_matrix_glove_27b_100d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.100d.txt\", 100)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_100d\", 100, embedding_matrix_glove_27b_100d, False)\n",
    "# embedding_matrix_glove_27b_200d =  construct_embedding_matrix(\"embeddings/glove.twitter.27B.200d.txt\", 200)\n",
    "# train_model(\"simple_lstm_glove_twitter_vectors_27B_200d\", 200, embedding_matrix_glove_27b_200d, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_lstm_baseline_100d_frozen.h5\n",
      "119/119 [==============================] - 2s 12ms/step - loss: 0.7604 - categorical_accuracy: 0.6777\n",
      "simple_lstm_baseline_100d_frozen.h5 [0.7604072690010071, 0.6777251362800598] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_100d.h5\n",
      "119/119 [==============================] - 2s 12ms/step - loss: 0.6169 - categorical_accuracy: 0.7586\n",
      "simple_lstm_glove_twitter_vectors_27B_100d.h5 [0.6169474720954895, 0.758557140827179] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_200d.h5\n",
      "119/119 [==============================] - 2s 14ms/step - loss: 0.5973 - categorical_accuracy: 0.7773\n",
      "simple_lstm_glove_twitter_vectors_27B_200d.h5 [0.5973194241523743, 0.7772511839866638] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_25d.h5\n",
      "119/119 [==============================] - 2s 12ms/step - loss: 0.7813 - categorical_accuracy: 0.6569\n",
      "simple_lstm_glove_twitter_vectors_27B_25d.h5 [0.7813110947608948, 0.6569247245788574] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_50d.h5\n",
      "119/119 [==============================] - 2s 12ms/step - loss: 0.6743 - categorical_accuracy: 0.7259\n",
      "simple_lstm_glove_twitter_vectors_27B_50d.h5 [0.674271821975708, 0.7259083986282349] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_100d.h5\n",
      "119/119 [==============================] - 2s 12ms/step - loss: 0.6267 - categorical_accuracy: 0.7522\n",
      "simple_lstm_glove_vectors_6B_100d.h5 [0.626747190952301, 0.7522380352020264] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_200d.h5\n",
      "119/119 [==============================] - 2s 14ms/step - loss: 0.6331 - categorical_accuracy: 0.7678\n",
      "simple_lstm_glove_vectors_6B_200d.h5 [0.6331303119659424, 0.7677724957466125] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_300d.h5\n",
      "119/119 [==============================] - 2s 19ms/step - loss: 0.5879 - categorical_accuracy: 0.7675\n",
      "simple_lstm_glove_vectors_6B_300d.h5 [0.5879257917404175, 0.7675092220306396] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_50d.h5\n",
      "119/119 [==============================] - 2s 11ms/step - loss: 0.6977 - categorical_accuracy: 0.7051\n",
      "simple_lstm_glove_vectors_6B_50d.h5 [0.6976825594902039, 0.7051079273223877] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Frozen models\n",
    "directory = os.fsencode(\"models_frozen/\")\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "     filename = os.fsdecode(file)\n",
    "     print(filename)\n",
    "     if filename.startswith(\"simple_lstm\"):\n",
    "         model = keras.models.load_model(\"models_frozen/\" + filename)\n",
    "         eval_ = model.evaluate(test_data, test_label)\n",
    "         print(filename, eval_, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple_lstm_baseline_100d.h5\n",
      "119/119 [==============================] - 2s 13ms/step - loss: 0.5816 - categorical_accuracy: 0.7817\n",
      "simple_lstm_baseline_100d.h5 [0.581606924533844, 0.7817272543907166] \n",
      "\n",
      "simple_lstm_ff_subwords_vectors_600B_300d.h5\n",
      "119/119 [==============================] - 2s 19ms/step - loss: 0.4927 - categorical_accuracy: 0.8394\n",
      "simple_lstm_ff_subwords_vectors_600B_300d.h5 [0.4926719069480896, 0.8393891453742981] \n",
      "\n",
      "simple_lstm_ff_vectors_600B_300d.h5\n",
      "119/119 [==============================] - 3s 20ms/step - loss: 0.4884 - categorical_accuracy: 0.8315 1s - loss:\n",
      "simple_lstm_ff_vectors_600B_300d.h5 [0.48840096592903137, 0.8314902782440186] \n",
      "\n",
      "simple_lstm_ff_wiki_subwords_vectors_16B_300d.h5\n",
      "119/119 [==============================] - 3s 22ms/step - loss: 0.5232 - categorical_accuracy: 0.8120\n",
      "simple_lstm_ff_wiki_subwords_vectors_16B_300d.h5 [0.5231635570526123, 0.8120062947273254] \n",
      "\n",
      "simple_lstm_ff_wiki_vectors_16B_300d.h5\n",
      "119/119 [==============================] - 3s 21ms/step - loss: 0.4774 - categorical_accuracy: 0.8323\n",
      "simple_lstm_ff_wiki_vectors_16B_300d.h5 [0.4773871898651123, 0.832280158996582] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_100d.h5\n",
      "119/119 [==============================] - 2s 13ms/step - loss: 0.4734 - categorical_accuracy: 0.8404\n",
      "simple_lstm_glove_twitter_vectors_27B_100d.h5 [0.47339096665382385, 0.8404423594474792] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_200d.h5\n",
      "119/119 [==============================] - 2s 15ms/step - loss: 0.4875 - categorical_accuracy: 0.8344\n",
      "simple_lstm_glove_twitter_vectors_27B_200d.h5 [0.4875066578388214, 0.8343865275382996] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_25d.h5\n",
      "119/119 [==============================] - 2s 12ms/step - loss: 0.4866 - categorical_accuracy: 0.8339\n",
      "simple_lstm_glove_twitter_vectors_27B_25d.h5 [0.4865739643573761, 0.833859920501709] \n",
      "\n",
      "simple_lstm_glove_twitter_vectors_27B_50d.h5\n",
      "119/119 [==============================] - 2s 12ms/step - loss: 0.4792 - categorical_accuracy: 0.8331\n",
      "simple_lstm_glove_twitter_vectors_27B_50d.h5 [0.4792165458202362, 0.8330700397491455] \n",
      "\n",
      "simple_lstm_glove_vectors_42B_300d.h5\n",
      "119/119 [==============================] - 3s 22ms/step - loss: 0.4813 - categorical_accuracy: 0.8368\n",
      "simple_lstm_glove_vectors_42B_300d.h5 [0.4812653064727783, 0.83675616979599] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_100d.h5\n",
      "119/119 [==============================] - 2s 13ms/step - loss: 0.4822 - categorical_accuracy: 0.8365\n",
      "simple_lstm_glove_vectors_6B_100d.h5 [0.4821579158306122, 0.8364928960800171] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_200d.h5\n",
      "119/119 [==============================] - 2s 16ms/step - loss: 0.4834 - categorical_accuracy: 0.8360\n",
      "simple_lstm_glove_vectors_6B_200d.h5 [0.48337334394454956, 0.8359662890434265] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_300d.h5\n",
      "119/119 [==============================] - 3s 21ms/step - loss: 0.4684 - categorical_accuracy: 0.8389\n",
      "simple_lstm_glove_vectors_6B_300d.h5 [0.4684218168258667, 0.8388625383377075] \n",
      "\n",
      "simple_lstm_glove_vectors_6B_50d.h5\n",
      "119/119 [==============================] - 2s 13ms/step - loss: 0.4810 - categorical_accuracy: 0.8270\n",
      "simple_lstm_glove_vectors_6B_50d.h5 [0.4809766113758087, 0.8270142078399658] \n",
      "\n",
      "simple_lstm_glove_vectors_840B_300d.h5\n",
      "119/119 [==============================] - 3s 24ms/step - loss: 0.4923 - categorical_accuracy: 0.8268\n",
      "simple_lstm_glove_vectors_840B_300d.h5 [0.4923143982887268, 0.8267509341239929] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuned models\n",
    "directory = os.fsencode(\"models/\")\n",
    "    \n",
    "for file in os.listdir(directory):\n",
    "     filename = os.fsdecode(file)\n",
    "     print(filename)\n",
    "     if filename.startswith(\"simple_lstm\"):\n",
    "         model = keras.models.load_model(\"models/\" + filename)\n",
    "         eval_ = model.evaluate(test_data, test_label)\n",
    "         print(filename, eval_, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground\n",
    "\n",
    "This code is not supposed to be run and is not a part of the report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"simple_lstm_glove_vectors_6B_50d.h5\")\n",
    "eval_ = model.evaluate(test_data, test_label)\n",
    "print(eval_)\n",
    "\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "print(words_embeddings['comfortable'])  # possible output: [0.21, 0.56, ..., 0.65, 0.10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 3s 23ms/step - loss: 0.4923 - categorical_accuracy: 0.8268\n",
      "[0.4923143982887268, 0.8267509341239929]\n",
      "[ 7.41575480e-01  8.57637003e-02 -4.92752373e-01  1.94836524e-03\n",
      "  1.49490684e-01  5.73379882e-02  3.67144674e-01 -4.89030123e-01\n",
      "  9.86776724e-02  1.96871054e+00  2.67749012e-01  4.09098044e-02\n",
      " -1.46645188e-01  6.42165601e-01 -2.03620702e-01 -4.04048920e-01\n",
      " -2.79254794e-01  1.53178358e+00  1.54406324e-01 -1.75111685e-02\n",
      " -1.17944813e+00  1.16829328e-01  7.83994198e-02 -2.76060760e-01\n",
      " -4.15574461e-01  1.95002183e-01 -1.46302264e-02 -2.73340702e-01\n",
      "  1.58137619e-01 -1.46372154e-01  2.90539920e-01 -1.42981574e-01\n",
      " -5.72584510e-01 -4.41698939e-01  3.27710241e-01 -3.34863394e-01\n",
      "  2.76684403e-01 -1.06199399e-01 -6.16275311e-01 -9.20918941e-01\n",
      "  3.44419926e-01 -6.69008866e-02 -5.03882468e-01  1.51864365e-01\n",
      "  1.18802726e-01  2.89673746e-01 -1.50014997e-01  1.13732576e-01\n",
      " -2.54492432e-01 -9.76564661e-02 -6.10187590e-01  2.82708704e-01\n",
      "  1.85410336e-01 -5.60531199e-01  4.54926103e-01  1.84068218e-01\n",
      "  4.27523553e-02 -6.62487745e-02  1.90614715e-01  6.53304517e-01\n",
      "  3.29316586e-01 -6.42563522e-01  6.36885613e-02  2.45411783e-01\n",
      " -1.90620705e-01 -3.32363322e-02 -1.49752975e-01  2.06934676e-01\n",
      "  3.80514681e-01  2.20572487e-01  8.59243423e-02  4.47149109e-03\n",
      "  1.56232268e-01 -1.29310176e-01 -6.07650355e-02 -3.69294316e-01\n",
      "  5.21823943e-01 -1.84172541e-01 -3.54868621e-02  4.01770800e-01\n",
      "  1.50888637e-01 -7.32928962e-02  2.75895238e-01  4.26642507e-01\n",
      " -3.47662240e-01 -1.00140483e-03 -2.73727179e-01  1.15734029e+00\n",
      "  6.92882657e-01 -1.94580890e-02  4.06581283e-01  1.80254027e-01\n",
      " -2.85369843e-01  3.28571051e-01 -3.20330113e-02 -3.26949328e-01\n",
      " -5.48612699e-02 -6.85818076e-01 -2.48176336e-01 -2.53383666e-01\n",
      "  5.49637616e-01  4.86670375e-01 -3.90281439e-01  1.93313062e-01\n",
      "  2.92375177e-01 -1.24121904e+00  3.12060058e-01 -2.64125049e-01\n",
      " -2.16444150e-01 -1.52606204e-01  5.14197886e-01 -1.85153827e-01\n",
      "  3.50678861e-01 -1.36879282e-02 -3.83552581e-01 -3.38737696e-01\n",
      " -1.21454962e-01 -4.48261142e-01 -9.06322524e-02 -4.76964891e-01\n",
      "  3.56106371e-01 -2.03221440e-01  1.88130185e-01  1.75170138e-01\n",
      "  3.70329887e-01 -3.70001018e-01  6.89803213e-02 -5.14699578e-01\n",
      "  3.87712538e-01 -6.54921830e-01  3.35312029e-03 -9.63116512e-02\n",
      " -9.80952382e-01 -3.22726429e-01  2.74170786e-02  4.51243877e-01\n",
      "  2.34707534e-01 -7.74617717e-02 -7.98607320e-02 -3.47785443e-01\n",
      " -1.23170197e+00  8.28302726e-02  2.82609403e-01  1.51229799e-01\n",
      "  1.80315256e-01  3.22371349e-02 -2.29489788e-01 -2.09349692e-01\n",
      "  6.72682226e-01 -3.00771266e-01 -5.05103171e-02  7.47555820e-03\n",
      " -1.65589973e-01 -6.84801459e-01  9.28857476e-02  5.98665118e-01\n",
      " -2.12321710e-02 -4.02422071e-01 -2.57177562e-01 -6.96484983e-01\n",
      " -4.05450732e-01 -9.71821994e-02  3.91102850e-01 -4.19324100e-01\n",
      "  8.55793238e-01 -5.77163577e-01  2.13722870e-01 -3.90306413e-02\n",
      " -9.68645960e-02 -5.04604340e-01 -2.01474845e-01  6.34998828e-02\n",
      "  8.26857910e-02  2.09350809e-01  3.78225476e-01 -6.06801696e-02\n",
      "  3.97545286e-02 -1.68505073e-01 -9.34446380e-02  1.79166585e-01\n",
      " -1.58885524e-01 -8.10258865e-01 -1.53500155e-01 -1.17061876e-01\n",
      "  3.36873591e-01 -1.40085131e-01  4.70452428e-01  1.91633508e-01\n",
      "  3.45455766e-01  4.94548008e-02 -4.01519537e-01 -8.99095908e-02\n",
      "  4.68586609e-02 -1.11278698e-01  8.71932656e-02 -5.30880570e-01\n",
      "  2.07370356e-01 -2.48743594e-01 -2.64210761e-01  5.68235636e-01\n",
      "  3.05871218e-01  1.77473396e-01  1.22486457e-01 -2.43609443e-01\n",
      "  4.16034788e-01  5.04288197e-01 -6.51301146e-01 -2.21682504e-01\n",
      " -6.36110961e-01 -3.07081759e-01 -7.98660576e-01 -2.87166804e-01\n",
      " -7.53422752e-02 -4.40702260e-01  7.24482417e-01  6.78469479e-01\n",
      "  3.65192562e-01 -4.72328775e-02 -4.14007872e-01  1.14427179e-01\n",
      "  2.48188302e-01  3.17835016e-04  2.49353379e-01  1.42146483e-01\n",
      "  2.39643425e-01 -2.21967191e-01 -3.71729583e-01  9.08250958e-02\n",
      "  2.29519561e-01  4.70541805e-01  3.92742336e-01 -1.65486798e-01\n",
      " -2.34444320e-01  1.77933052e-01  3.68246287e-01 -1.83967486e-01\n",
      "  2.83403754e-01 -6.76462173e-01 -3.81888777e-01  1.99236795e-01\n",
      " -2.11253107e-01 -3.68307382e-01 -3.34350616e-01  6.67828143e-01\n",
      "  8.01572055e-02  1.80332407e-01 -8.78557742e-01 -5.75778842e-01\n",
      " -2.98751891e-01  5.43491185e-01  9.51986238e-02 -1.75423305e-02\n",
      " -8.66208673e-02  5.67637384e-01 -6.35043561e-01  9.32331085e-01\n",
      "  5.44484138e-01  1.59880280e-01 -3.25447768e-01  5.34100890e-01\n",
      " -2.49784678e-01  2.62003958e-01  7.63025522e-01 -1.56673267e-01\n",
      "  2.40303397e-01  1.45870045e-01 -1.00810669e-01  2.01365501e-01\n",
      " -7.41725147e-01  1.67278171e-01 -3.78384888e-01 -6.12693369e-01\n",
      " -6.38603866e-01 -2.32645810e-01 -3.52177262e-01  2.15409920e-01\n",
      "  9.36532497e-01 -1.99330449e-02 -6.14485554e-02  1.52386948e-01\n",
      "  6.66828230e-02 -2.23901793e-01 -4.42957669e-01  2.64486879e-01\n",
      " -3.82190406e-01 -4.38185215e-01 -1.03553019e-01  4.23446029e-01\n",
      "  1.41191855e-01  1.40376523e-01  2.82358855e-01  3.56762737e-01\n",
      " -1.13730572e-01 -3.71109277e-01  4.47358757e-01  4.42255735e-01\n",
      "  1.37773976e-01  3.17929357e-01  1.40813202e-01 -3.17992866e-02]\n",
      "119/119 [==============================] - 3s 24ms/step - loss: 0.4813 - categorical_accuracy: 0.8368\n",
      "[0.4812653064727783, 0.83675616979599]\n",
      "[ 1.10442184e-01  2.31265590e-01  2.16376353e-02 -4.91351426e-01\n",
      "  1.11669540e-01 -5.54120950e-02 -3.14222693e+00 -8.62034559e-01\n",
      " -8.26451331e-02 -6.10853136e-01 -1.27499670e-01 -5.13938487e-01\n",
      "  3.94721627e-01  2.98315614e-01 -5.46521544e-01 -9.66614708e-02\n",
      "  2.02009410e-01 -3.39163750e-01 -6.06669188e-01 -6.27955794e-01\n",
      " -1.71762839e-01  1.51620694e-02 -5.38890719e-01  3.65640879e-01\n",
      "  1.95646416e-02  8.65923688e-02  2.47097269e-01  4.00709867e-01\n",
      "  3.28453362e-01 -1.86579466e-01 -5.92701018e-01  1.74542531e-01\n",
      "  1.49458140e-01  2.40142331e-01  7.64595151e-01 -8.26295376e-01\n",
      " -2.74997294e-01  2.84989238e-01 -2.94902474e-01 -1.80193394e-01\n",
      " -1.62440777e-01  3.48372832e-02  1.36495084e-01 -2.08326001e-02\n",
      "  2.41295442e-01 -4.04245287e-01  1.11562647e-01 -1.70019165e-01\n",
      " -1.58800319e-01  1.28378183e-01  5.55920124e-01  1.35951012e-01\n",
      " -5.22392690e-01 -3.83357823e-01  6.32058084e-01  9.01212320e-02\n",
      "  1.34548903e-01 -7.95131862e-01  2.35554646e-04 -5.36566019e-01\n",
      "  1.96835086e-01  8.16182643e-02  2.42779255e-01 -3.33865494e-01\n",
      "  4.35037941e-01 -4.95736688e-01  3.06584477e-01  8.57383788e-01\n",
      "  3.61348838e-01 -4.71287638e-01  4.34239715e-01 -5.68130985e-02\n",
      " -4.81495231e-01 -9.63556170e-02 -3.24292868e-01 -5.00005603e-01\n",
      " -1.72893214e-03  4.02659297e-01  5.10755181e-01  1.24065012e-01\n",
      "  7.51674250e-02  1.11361837e+00 -2.17330411e-01 -4.24865693e-01\n",
      " -1.34893298e-01  4.21431899e-01  1.30848661e-01 -3.07234406e-01\n",
      " -2.98672300e-02  1.22053862e-01 -3.03536534e-01  2.06031412e-01\n",
      " -3.09277866e-02  3.45703274e-01  1.53766274e-02 -1.57875061e-01\n",
      " -2.10200691e+00  3.65404576e-01 -2.08135545e-02  2.72949010e-01\n",
      " -2.68793805e-03 -1.15463054e-02 -4.06324714e-02 -2.31594458e-01\n",
      "  1.16626278e-01  2.87599295e-01 -7.07392573e-01 -2.20894530e-01\n",
      " -7.88178295e-02 -1.76300421e-01 -1.08049579e-01 -1.33953854e-01\n",
      "  5.79106286e-02 -2.23567948e-01 -4.12297249e-01  1.33387581e-01\n",
      " -1.39144570e-01 -1.32442445e-01  6.80504382e-01 -3.46571594e-01\n",
      "  7.45007098e-01  1.91094652e-01  1.48068935e-01 -3.16555142e-01\n",
      "  5.85041583e-01 -2.27648214e-01  1.42043635e-01 -2.12370098e-01\n",
      " -1.94970056e-01  3.49054456e-01 -2.63404191e-01  1.28740266e-01\n",
      " -4.65801537e-01 -6.00150168e-01 -4.63890731e-01  2.29374528e-01\n",
      " -1.58901602e-01  1.96915135e-01  3.55547518e-01 -7.41856754e-01\n",
      " -2.31517293e-02 -1.02613933e-01  3.48176807e-01 -3.12986881e-01\n",
      " -3.36728960e-01  5.51799059e-01 -4.38948184e-01  7.80203044e-02\n",
      "  1.86950147e-01 -9.68021154e-03  2.69041091e-01  2.40998454e-02\n",
      "  6.61096647e-02  2.50224620e-02  4.08593118e-01  4.11820292e-01\n",
      " -2.28657648e-01 -8.75367895e-02 -5.40958583e-01  5.87113738e-01\n",
      "  5.24869919e-01  3.76062915e-02 -7.41071580e-03  7.30723515e-02\n",
      " -1.71861053e-01  8.19240570e-01  3.02120715e-01 -2.86732763e-01\n",
      "  1.41216978e-01 -1.69960573e-01  8.97223037e-03 -9.42638144e-02\n",
      " -3.55108567e-02 -1.57113507e-01 -3.40662897e-01 -1.33043155e-01\n",
      "  2.72938609e-01 -4.79073882e-01 -2.38752082e-01 -1.53765827e-01\n",
      " -1.22084945e-01 -3.09464596e-02 -1.08367845e-01  4.35036242e-01\n",
      "  1.45359775e-02  8.21819365e-01 -2.60187596e-01  1.75484076e-01\n",
      " -5.40757537e-01  4.07565311e-02 -3.21051955e-01  2.31483057e-01\n",
      " -1.07791543e+00  4.35270965e-01  1.19725183e-01 -1.72745422e-01\n",
      "  5.40652156e-01 -2.79934585e-01  2.44632453e-01  4.23656493e-01\n",
      "  1.14557767e+00 -4.66164619e-01  1.18691064e-01 -3.67378682e-01\n",
      "  3.35912779e-02  5.74867241e-02  1.79914176e-01 -1.93022013e-01\n",
      "  6.02286518e-01 -5.36039829e-01 -3.93137604e-01  2.97509774e-04\n",
      " -7.12376386e-02 -2.33385965e-01  5.77206314e-01  8.63197520e-02\n",
      " -4.21478450e-01  4.10780936e-01  7.69353583e-02 -1.22765076e+00\n",
      " -3.47305030e-01 -2.85559762e-02  6.67675138e-01 -5.41519761e-01\n",
      " -1.87700939e+00 -6.54152334e-01 -2.37862259e-01 -3.26231897e-01\n",
      " -2.14453459e-01 -3.49130869e-01  2.50949621e-01  4.49065603e-02\n",
      " -5.68485022e-01 -1.41271710e-01 -9.22595412e-02  2.48601846e-02\n",
      " -1.13654183e-02  4.11946267e-01  6.10549510e-01  4.63662058e-01\n",
      " -5.55797480e-02 -7.09051639e-02  7.95449913e-02  1.04250662e-01\n",
      "  2.06458926e-01 -6.07188120e-02  1.84149995e-01 -2.20985815e-01\n",
      "  7.46905953e-02  2.02295199e-01  7.53265917e-01 -7.79492483e-02\n",
      " -2.55737692e-01 -3.64100873e-01 -1.12661898e-01  2.41441101e-01\n",
      "  1.52737096e-01 -2.44933460e-02  6.55936599e-02  3.06978226e-01\n",
      "  7.39228278e-02  9.64712203e-02  3.00390143e-02 -3.26444775e-01\n",
      "  3.61592919e-01 -2.23568529e-01 -2.89835483e-01  1.37188107e-01\n",
      "  2.61020511e-01  6.39667809e-02  1.57083824e-01  5.21746695e-01\n",
      " -1.00824547e+00 -2.68786132e-01 -3.20189446e-01 -1.88122928e-01\n",
      " -4.80837315e-01  2.87117451e-01 -2.60234356e-01  2.83574164e-01\n",
      "  4.34325337e-01 -2.91925132e-01  3.14695269e-01 -1.88261271e-02\n",
      "  4.55602139e-01  4.82758522e-01  1.44563764e-01  8.48376229e-02\n",
      " -2.26484075e-01 -6.76067472e-01 -4.05275114e-02 -3.54264915e-01\n",
      " -9.38616872e-01 -1.19692581e-02  3.48587722e-01 -1.82483852e-01\n",
      " -3.24766845e-01 -1.96848035e-01  2.22612664e-01 -5.84698617e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.584659"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"models/simple_lstm_glove_vectors_840B_300d.h5\")\n",
    "eval_ = model.evaluate(test_data, test_label)\n",
    "print(eval_)\n",
    "\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "v1 = words_embeddings['comfortable']\n",
    "print(words_embeddings['comfortable'])\n",
    "\n",
    "\n",
    "model = keras.models.load_model(\"models/simple_lstm_glove_vectors_42B_300d.h5\")\n",
    "eval_ = model.evaluate(test_data, test_label)\n",
    "print(eval_)\n",
    "\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}\n",
    "v2 = words_embeddings['comfortable']\n",
    "print(words_embeddings['comfortable'])\n",
    "np.linalg.norm(v1-v2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.8787320e-01 4.2825625e-03 7.8442590e-03]\n",
      " [1.5778446e-03 8.3056418e-03 9.9011654e-01]\n",
      " [5.9161469e-04 3.2535938e-03 9.9615484e-01]\n",
      " ...\n",
      " [4.7244359e-02 9.4788247e-01 4.8732213e-03]\n",
      " [9.9504423e-01 1.9307305e-03 3.0250535e-03]\n",
      " [9.2169025e-04 2.9376131e-03 9.9614066e-01]]\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(test_data)\n",
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "27f9f31083b1491bb657d358dd9ef3834cbda27041550bdcb5ccfa2b16bbb078"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('env-tdde16-project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
