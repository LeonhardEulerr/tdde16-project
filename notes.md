- perform lemmatization for naive bayes
- make sure you understand how word embeddings are created and how they influance the LSTM
- what is an adventage of using word embeddings for LSTM vs training from scratch (https://towardsdatascience.com/pre-trained-word-embeddings-or-embedding-layer-a-dilemma-8406959fd76c)
- use just embedding layer in LSTM and see how is the result
- use spacy, word2vec, gloVe, fast Text word embeddings and see which one gives best result for LSTM
-

# Questions to Marco

- I am about to do a sentiment analysis where I was to compare naive Bayes with LSTM, we figured out that it might be to little so we talked about adding how pre-trained word embeddings influence the result in LSTM.

1. Should I use BERT also and compare the results?
2. I am also wondering if I should do the naive bayes part or should I just treat LSTM with embedding layer as baseline?
